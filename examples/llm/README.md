# Naylence Agent SDK — LLM Examples (Single‑Process)

This set of examples demonstrates how to use the Naylence Agent SDK to wrap **LLMs and multimodal APIs** as agents, while still running agent and client in the same process (no sentinel required). They highlight text chat, Q\&A, and image generation with OpenAI APIs.

> ⚠️ **Security note:** These examples are intentionally **insecure and non‑distributed**. They are for learning purposes only. Later distributed examples introduce admission via sentinels, and later still add authentication, identities, and overlay security.

---

## What you’ll learn

* How to wrap an **async function** into an agent (`Agent.from_handler`).
* How to implement an agent class that maintains **conversation state** across sessions.
* How to integrate with the **OpenAI Chat** and **Images** APIs.
* How to **stream results** and handle task payloads.
* How to save and view generated images.

---

## Example catalog

| File                        | Concept                      | What it does                                                                                                                         |
| --------------------------- | ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| `llm_agent.py`              | **Function as Agent (Q\&A)** | Wraps an async function using `Agent.from_handler`; asks “What year did the first moon landing occur?” via GPT.                      |
| `chat_agent.py`             | **Chat Agent with Memory**   | A `BaseAgent` subclass that maintains per‑session history; supports multi‑turn conversations with GPT.                               |
| `image_generation_agent.py` | **Image Generation Agent**   | Calls OpenAI’s DALL‑E model to generate images. Enhances prompts with GPT‑4.1‑mini, saves PNGs locally, optionally opens in browser. |
| `view_images.py`            | **Simple HTTP Viewer**       | Serves `generated_images/` on `http://localhost:8000` to browse images generated by the agent.                                       |

---

## Prerequisites

* **Python 3.11+**
* Install requirements:

  ```bash
  pip install openai pillow numpy anthropic tiktoken python-dotenv
  ```
* Set your OpenAI API key:

  ```bash
  export OPENAI_API_KEY="sk-…"
  ```

---

## Running the examples (local Python)

Run any script directly:

```bash
python llm_agent.py
python chat_agent.py
python image_generation_agent.py --prompt "A dragon curled around a mountain" --size 512x512
```

### Expected behaviors

* **llm\_agent.py** → prints a fixed Q\&A (moon landing year).
* **chat\_agent.py** → starts a REPL; type questions, get GPT responses; type `exit` to quit.
* **image\_generation\_agent.py** → generates an image, saves under `generated_images/`, prints path, optionally opens in browser.
* **view\_images.py** → run after generating images to view them in your browser.

---

## Running with Docker Compose

A `docker-compose.yml` is provided to run agents in isolated containers. Example:

```bash
docker compose up llm-agent
```

This runs `llm_agent.py` inside the base image with your API key injected.

The `image-agent` service mounts `./generated_images` to persist images.

---

## How it works

* **Agents** wrap OpenAI calls (`AsyncOpenAI` for chat, `images.generate` for images).
* **Fabric** is created locally: `async with FameFabric.create(): …`
* **Serving:** `agent_addr = await fabric.serve(MyAgent())`
* **Client proxy:** `remote = Agent.remote_by_address(agent_addr)`
* **Calling:** `await remote.run_task(payload=…)` or `await remote.start_task(…)`

Everything runs in one process, so the fabric routes requests in‑memory.

---

## Troubleshooting

* **Missing API key** → set `OPENAI_API_KEY` in your shell or `.env`.
* **Image not opening** → pass `--no-browser` when running in Docker/headless environments.
* **HTTP viewer not loading** → ensure you generated images first and that port 8000 is free.

---

## Next steps

* Run the **distributed echo example** (sentinel + agent + client) to see real fabric connections.
* Explore **security‑enabled** examples with signed envelopes, overlay encryption, and SPIFFE‑style identities.
* Extend `chat_agent.py` with your own conversation logic or replace `image_generation_agent.py` with another multimodal model.

---

These LLM examples show how Naylence agents can encapsulate external model APIs. They’re designed as a bridge from **local prototyping** to **distributed and secure multi‑agent systems**.
