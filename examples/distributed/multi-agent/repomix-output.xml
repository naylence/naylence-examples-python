This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
analysis_agent.py
client.py
common.py
docker-compose.yml
Dockerfile
README.md
sentiment_agent.py
sentinel.py
summarizer_agent.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="analysis_agent.py">
import asyncio
from typing import Any

from common import ANALYSIS_AGENT_ADDR, SENTIMENT_AGENT_ADDR, SUMMARIZER_AGENT_ADDR

from naylence.agent import Agent, BaseAgent, dev_mode


class AnalysisAgent(BaseAgent):
    async def run_task(
        self,
        payload: dict[str, Any] | str | None,
        id: str | None,
    ) -> dict[str, Any] | str:
        result = await Agent.broadcast(
            [SUMMARIZER_AGENT_ADDR, SENTIMENT_AGENT_ADDR], payload
        )
        return {
            "summary": result[0][1],
            "sentiment": result[1][1],
        }


if __name__ == "__main__":
    asyncio.run(
        AnalysisAgent().aserve(
            ANALYSIS_AGENT_ADDR, root_config=dev_mode.NODE_CONFIG, log_level="info"
        )
    )
</file>

<file path="client.py">
import asyncio
import json

from common import ANALYSIS_AGENT_ADDR
from naylence.fame.core import FameFabric

from naylence.agent import Agent, dev_mode

text_to_analyze = """
    I just watched the new sci-fi film “Galactic Frontier” and I have mixed feelings.
    The visuals were stunning and the world-building immersive, but the plot felt predictable
    and some characters lacked depth. Overall, it was an entertaining experience but not
    groundbreaking.
"""


async def main():
    async with FameFabric.create(root_config=dev_mode.CLIENT_CONFIG):
        agent = Agent.remote_by_address(ANALYSIS_AGENT_ADDR)
        result = await agent.run_task(payload=text_to_analyze)
        print(json.dumps(result, indent=2))


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="common.py">
import os

ANALYSIS_AGENT_ADDR = "analysis@fame.fabric"
SUMMARIZER_AGENT_ADDR = "summarizer@fame.fabric"
SENTIMENT_AGENT_ADDR = "sentiment@fame.fabric"


def get_openai_client():
    from openai import AsyncOpenAI

    openai_api_key = os.getenv("OPENAI_API_KEY")
    return AsyncOpenAI(api_key=openai_api_key)


def get_model_name():
    return os.getenv("MODEL_NAME") or "gpt-4.1-mini"
</file>

<file path="docker-compose.yml">
x-images: &images
  base: &base-image 
    build: .

services:
  # Sentinel service - runs the central coordinator on port 8000
  sentinel:
    build: .
    volumes:
      - .:/work:ro
    working_dir: /work
    command: ["python", "sentinel.py"]
    ports:
      - "8000:8000"
    networks:
      - naylence-net
    stop_signal: SIGINT
    stop_grace_period: 1s
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s=socket.socket(); s.connect(('localhost', 8000)); s.close()"]
      interval: 0.5s
      timeout: 1s
      retries: 10
      start_period: 0.5s
      start_interval: 1s

  # Analysis Agent service - connects to sentinel and provides analysis functionality
  analysis-agent:
    build: .
    volumes:
      - .:/work:ro
    working_dir: /work
    command: ["python", "analysis_agent.py"]
    depends_on:
      sentinel:
        condition: service_healthy
    networks:
      - naylence-net
    environment:
      - FAME_DIRECT_ADMISSION_URL=ws://sentinel:8000/fame/v1/attach/ws/downstream
      - OPENAI_API_KEY=${OPENAI_API_KEY}

    restart: unless-stopped


  # Sentiment Agent service - connects to sentinel and provides sentiment analysis functionality
  sentiment-agent:
    build: .
    volumes:
      - .:/work:ro
    working_dir: /work
    command: ["python", "sentiment_agent.py"]
    depends_on:
      sentinel:
        condition: service_healthy
    networks:
      - naylence-net
    environment:
      - FAME_DIRECT_ADMISSION_URL=ws://sentinel:8000/fame/v1/attach/ws/downstream
      - OPENAI_API_KEY=${OPENAI_API_KEY}

    restart: unless-stopped


  # Sentiment Agent service - connects to sentinel and provides summarization
  summarizer-agent:
    build: .
    volumes:
      - .:/work:ro
    working_dir: /work
    command: ["python", "summarizer_agent.py"]
    depends_on:
      sentinel:
        condition: service_healthy
    networks:
      - naylence-net
    environment:
      - FAME_DIRECT_ADMISSION_URL=ws://sentinel:8000/fame/v1/attach/ws/downstream
      - OPENAI_API_KEY=${OPENAI_API_KEY}

    restart: unless-stopped

networks:
  naylence-net:
    driver: bridge
</file>

<file path="Dockerfile">
FROM ghcr.io/naylence/agent-sdk-base:0.1.8

# Install additional packages required for LLM examples
RUN pip install --no-cache-dir \
    openai

WORKDIR /work
</file>

<file path="README.md">
# Text Analysis Pipeline Example

This directory demonstrates a simple end-to-end text analysis pipeline built with the Naylence Agent SDK. It includes:

* **Fame router** (`sentinel.py`)
* **OpenAI-powered agents**:

  * `summarizer_agent.py` — Summarizes input text
  * `sentiment_agent.py` — Rates sentiment on a 1–5 scale
* **Orchestrator agent** (`analysis_agent.py`) — Runs summarization and sentiment analysis in parallel using `Agent.run_all`
* **Client script** (`client.py`) — Submits sample text and prints the combined result
* **Shared configuration** (`common_config.py`)

---

## Prerequisites

* **Python 3.12+**
* **Naylence Agent SDK** installed (editable or from PyPI):

  ```bash
  pip install naylence-agent-sdk
  ```
* **OpenAI SDK**:

  ```bash
  pip install openai
  ```
* Environment variables:

  * `OPENAI_API_KEY` — your OpenAI API key (required)
  * `MODEL_NAME` — (optional) defaults to `gpt-4.1-mini`

---

## Setup & Installation

1. Clone the repository and install dependencies:

   ```bash
   git clone <repo-url>
   cd naylence-agent-sdk
   pip install -e .[examples]
   pip install openai
   ```

2. (Optional) Verify your `OPENAI_API_KEY` is set:

   ```bash
   export OPENAI_API_KEY="sk-..."
   ```

---

## Running the Example

1. **Start the Fame router**

   ```bash
   cd examples
   python sentinel.py
   ```

   This launches a FastAPI app on `http://0.0.0.0:8000` and exposes the WS attach endpoint at `/fame/ws/downstream`.

2. **In separate terminals, launch the agents**

   ```bash
   # Summarizer
   python summarizer_agent.py

   # Sentiment
   python sentiment_agent.py

   # Orchestrator (AnalysisAgent)
   python analysis_agent.py
   ```

3. **Run the client script**

   ```bash
   python client.py
   ```

   You should see output like:

   ```text
   Analysis agent task completed with result: {
     "summarizer@fame.fabric": "<summary text>",
     "sentiment@fame.fabric": "3"
   }
   ```

---

## File Reference

| File                  | Description                                                                                |
| --------------------- | ------------------------------------------------------------------------------------------ |
| `common_config.py`    | Shared FameFabric configuration (`dev` mode, WS URL, logical roots)                        |
| `sentinel.py`         | Fame router (FastAPI + WS attach)                                                          |
| `summarizer_agent.py` | `SummarizerAgent` — uses OpenAI to summarize text                                          |
| `sentiment_agent.py`  | `SentimentAgent` — uses OpenAI to rate sentiment on a 1–5 scale                            |
| `analysis_agent.py`   | `AnalysisAgent` — calls summarizer & sentiment in parallel via `Agent.run_all`             |
| `client.py`           | Simple client that submits a sample passage to `analysis@/` and prints the combined output |

---

## Configuration

* Modify `common.py` to adjust your FameFabric settings (e.g., change `mode`, `direct_parent_url`).
* Set `MODEL_NAME` if you want to use a different OpenAI model:

  ```bash
  export MODEL_NAME="gpt-4.1-mini"
  ```

---

## Notes & Tips

* **Order matters**: Always start the router (`sentinel.py`) before launching agents.
* **Parallel dispatch**: `Agent.run_all` implements a scatter–gather pattern under the hood.
* **Error handling**: If OpenAI calls fail, check your API key and model name.
* **Extending**: Use this as a template to add more agents or different NLP tasks.
</file>

<file path="sentiment_agent.py">
import asyncio
from typing import Any

from common import SENTIMENT_AGENT_ADDR, get_model_name, get_openai_client

from naylence.agent import BaseAgent, dev_mode

client = get_openai_client()


class SentimentAgent(BaseAgent):
    async def run_task(
        self,
        payload: dict[str, Any] | str | None,
        id: str | None,
    ) -> dict[str, Any] | str:
        response = await client.chat.completions.create(
            model=get_model_name(),
            messages=[
                {
                    "role": "user",
                    "content": f"Rate the sentiment of this on a scale 1-5 (number only):\n\n{payload}",
                }
            ],
        )
        return response.choices[0].message.content.strip()  # type: ignore


if __name__ == "__main__":
    asyncio.run(
        SentimentAgent().aserve(
            SENTIMENT_AGENT_ADDR, root_config=dev_mode.NODE_CONFIG, log_level="info"
        )
    )
</file>

<file path="sentinel.py">
import asyncio
from naylence.fame.sentinel import Sentinel
from naylence.agent import dev_mode


if __name__ == "__main__":
    asyncio.run(Sentinel.aserve(root_config=dev_mode.SENTINEL_CONFIG, log_level="info"))
</file>

<file path="summarizer_agent.py">
import asyncio
from typing import Any

from common import SUMMARIZER_AGENT_ADDR, get_model_name, get_openai_client

from naylence.agent import BaseAgent, dev_mode


client = get_openai_client()


class SummarizerAgent(BaseAgent):
    async def run_task(
        self,
        payload: dict[str, Any] | str | None,
        id: str | None,
    ) -> dict[str, Any] | str:
        response = await client.chat.completions.create(
            model=get_model_name(),
            messages=[{"role": "user", "content": f"Summarize this:\n\n{payload}"}],
        )
        return response.choices[0].message.content.strip()  # type: ignore


if __name__ == "__main__":
    asyncio.run(
        SummarizerAgent().aserve(
            SUMMARIZER_AGENT_ADDR, root_config=dev_mode.NODE_CONFIG, log_level="info"
        )
    )
</file>

</files>
