# Multi‚ÄëAgent Text Analysis Pipeline

This example demonstrates a **distributed multi‚Äëagent pipeline** with Naylence. Multiple agents collaborate to analyze text: one agent summarizes, ano## Troubleshooting

* **Missing API key** ‚Üí **REQUIRED:** Set `OPENAI_API_KEY` in your shell before running. Get your key from https://platform.openai.com/api-keys
* **Agents don't connect** ‚Üí start sentinel first; ensure `FAME_DIRECT_ADMISSION_URL` is set.
* **Results empty** ‚Üí check OpenAI model availability; override with `MODEL_NAME` env var.
* **API quota exceeded** ‚Üí check your OpenAI account usage and billing at https://platform.openai.com/usageevaluates sentiment, and a third orchestrates them and combines results. A client then consumes the aggregated analysis.

> üîë **Requirements:** This example requires an **OpenAI API key** as the agents use OpenAI's API for text analysis.

```
request: client ‚îÄ‚îÄ‚ñ∂ sentinel ‚îÄ‚îÄ‚ñ∂ analysis-agent ‚îÄ‚îÄ‚î¨‚îÄ‚ñ∂ summarizer-agent
                                                  ‚îî‚îÄ‚ñ∂ sentiment-agent
reply:   client ‚óÄ‚îÄ sentinel ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ results merged
```

> ‚ö†Ô∏è **Security note:** This demo is intentionally **insecure** (no TLS, no identities, no overlay encryption). Later examples introduce secure admission, envelope signing, and overlay security.

---

## What you‚Äôll learn

* How to run multiple agents on the same fabric, each with its own logical address.
* How one agent (`AnalysisAgent`) can call others in parallel using **`Agent.broadcast`** (scatter‚Äìgather pattern).
* How to structure an orchestrator that merges outputs into a combined payload.

---

## Components

* **sentinel.py** ‚Äî runs the sentinel (fabric router at `:8000`).
* **summarizer\_agent.py** ‚Äî uses OpenAI to generate a summary of input text.
* **sentiment\_agent.py** ‚Äî uses OpenAI to score sentiment 1‚Äì5.
* **analysis\_agent.py** ‚Äî orchestrator; dispatches to summarizer & sentiment agents, collects results, returns combined object.
* **client.py** ‚Äî submits text to the analysis agent and prints JSON result.
* **common.py** ‚Äî shared addresses and OpenAI client setup.
* **docker-compose.yml** ‚Äî runs sentinel + three agents; client runs on host.

**Logical addresses**

* `summarizer@fame.fabric`
* `sentiment@fame.fabric`
* `analysis@fame.fabric`

---

## Quick start

> **Prerequisites:** 
> - Docker + Docker Compose installed
> - **OpenAI API key** (required for text analysis)

**First, set your OpenAI API key:**

```bash
export OPENAI_API_KEY="sk-..."  # ‚ö†Ô∏è REQUIRED - get from https://platform.openai.com/api-keys
```

From this example folder:

```bash
make start       # üöÄ brings up the stack (sentinel + three agents)
```

Run the sample client against the analysis agent:

```bash
make run         # ‚ñ∂Ô∏è executes client.py
```

Shut down when done:

```bash
make stop        # ‚èπ stop containers
```

### See envelope traffic

Use the verbose target to print every **envelope** as it travels through the fabric:

```bash
make run-verbose
```

---

## Alternative: Quick start (Docker Compose)

1. **Start services**

```bash
docker compose up -d
```

This launches:

* **sentinel** on `localhost:8000`
* **summarizer-agent**, **sentiment-agent**, and **analysis-agent** connected downstream.

2. **Run the client (host)**

```bash
export OPENAI_API_KEY="sk-..."  # ensure set
export FAME_DIRECT_ADMISSION_URL="ws://localhost:8000/fame/v1/attach/ws/downstream"
python client.py
```

### Example output

```json
{
  "summary": "The film Galactic Frontier dazzles visually but has a predictable plot and shallow characters.",
  "sentiment": "3"
}
```

3. **Stop services**

```bash
docker compose down --remove-orphans
```

---

## Standalone (no Compose)

Run each component in separate terminals:

**Terminal A ‚Äî sentinel**

```bash
python sentinel.py
```

**Terminal B ‚Äî summarizer**

```bash
export FAME_DIRECT_ADMISSION_URL="ws://localhost:8000/fame/v1/attach/ws/downstream"
python summarizer_agent.py
```

**Terminal C ‚Äî sentiment**

```bash
export FAME_DIRECT_ADMISSION_URL="ws://localhost:8000/fame/v1/attach/ws/downstream"
python sentiment_agent.py
```

**Terminal D ‚Äî analysis (orchestrator)**

```bash
export FAME_DIRECT_ADMISSION_URL="ws://localhost:8000/fame/v1/attach/ws/downstream"
python analysis_agent.py
```

**Terminal E ‚Äî client**

```bash
export FAME_DIRECT_ADMISSION_URL="ws://localhost:8000/fame/v1/attach/ws/downstream"
python client.py
```

---

## Code snippets

### AnalysisAgent orchestration

```python
class AnalysisAgent(BaseAgent):
    async def run_task(self, payload, id):
        result = await Agent.broadcast(
            [SUMMARIZER_AGENT_ADDR, SENTIMENT_AGENT_ADDR],
            payload
        )
        return {
            "summary": result[0][1],
            "sentiment": result[1][1],
        }
```

### Client call

```python
async with FameFabric.create(root_config=dev_mode.CLIENT_CONFIG):
    agent = Agent.remote_by_address(ANALYSIS_AGENT_ADDR)
    result = await agent.run_task(payload=text)
    print(json.dumps(result, indent=2))
```

---

## Troubleshooting

* **Missing API key** ‚Üí set `OPENAI_API_KEY` in your shell.
* **Agents don‚Äôt connect** ‚Üí start sentinel first; ensure `FAME_DIRECT_ADMISSION_URL` is set.
* **Results empty** ‚Üí check OpenAI model availability; override with `MODEL_NAME` env var.

---

## Next steps

* Add more specialized agents (e.g., keyword extractor, entity recognizer) and have the analysis agent orchestrate them.
* Chain agents sequentially (e.g., translation ‚Üí summarization ‚Üí sentiment).
* Add resilience (timeouts, retries, degraded results if one agent fails).
* Secure the pipeline with authenticated identities and overlay encryption.

---

This example shows how **multiple agents** can collaborate seamlessly in a distributed fabric, with orchestration logic living inside a higher‚Äëlevel agent rather than the client.
